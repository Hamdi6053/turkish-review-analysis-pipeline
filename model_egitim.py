import os
import pandas as pd
import numpy as np
import re  # Eklendi - metin temizleme iÃ§in
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.model_selection import StratifiedKFold
from imblearn.over_sampling import SMOTE
import joblib
import time
import warnings
warnings.filterwarnings('ignore')

# Define function outside of lambda for pickling compatibility
def make_dense(X):
    return X.toarray()

# ğŸ“ Excel klasÃ¶rÃ¼ - BURAYA EXCEL DOSYALARININ YOLUNU GÄ°RÄ°N
excel_folder = r"C:\Users\hamdi\Downloads\Exceller_Kategori_AtamalarÄ±"

# KlasÃ¶r yoksa hata ver
if not os.path.exists(excel_folder):
    print(f"âš ï¸ '{excel_folder}' klasÃ¶rÃ¼ bulunamadÄ±!")
    print("Excel klasÃ¶rÃ¼ yolunu doÄŸru ÅŸekilde gÃ¼ncellemeniz gerekiyor.")
    exit()

# TÃ¼rkÃ§e stop word listesi - GeniÅŸletildi ve daha agresif hale getirildi
turkish_stop_words = [
    've', 'bir', 'bu', 'iÃ§in', 'de', 'da', 'ne', 'veya', 'ile', 'mi', 'mu', 'mÃ¼',
    'ben', 'sen', 'o', 'biz', 'siz', 'onlar', 'ama', 'fakat', 'Ã§Ã¼nkÃ¼', 'gibi',
    'her', 'ÅŸey', 'Ã§ok', 'az', 'daha', 'en', 'ki', 'ise', 'olarak', 'kadar',
    'sonra', 'Ã¶nce', 'ancak', 'ama', 'fakat', 'lakin', 'yani', 'nasÄ±l',
    # Ek stop words - daha agresif filtreleme
    'var', 'yok', 'oldu', 'olur', 'etti', 'eder', 'etmek', 'olmak', 'yapmak',
    'yapÄ±yor', 'yapÄ±lÄ±r', 'eden', 'olan', 'iÃ§inde', 'Ã¼zerinde', 'altÄ±nda',
    'yanÄ±nda', 'dÄ±ÅŸÄ±nda', 'karÅŸÄ±', 'doÄŸru', 'gÃ¶re', 'raÄŸmen', 'dolayÄ±',
    'nedeniyle', 'yÃ¼zÃ¼nden', 'sayesinde', 'tarafÄ±ndan', 'itibaren', 'Ã¶nce',
    'sonra', 'kadar', 'sÄ±rasÄ±nda', 'boyunca', 'kez', 'defa', 'kere', 'birÃ§ok',
    'birkaÃ§', 'bazÄ±', 'tÃ¼m', 'bÃ¼tÃ¼n', 'tamam', 'tamamen', 'kÄ±smen', 'evet',
    'hayÄ±r', 'belki', 'galiba', 'sanÄ±rÄ±m', 'herhalde', 'kesinlikle', 'mutlaka'
]

# Hedef doÄŸruluk - %80-85 arasÄ± (daha yÃ¼ksek aÅŸÄ±rÄ± Ã¶ÄŸrenmedir)
TARGET_ACCURACY_MIN = 0.80
TARGET_ACCURACY_MAX = 0.85

# EÄŸitim ve test seti arasÄ±ndaki maksimum kabul edilebilir fark
MAX_ACC_DIFF = 0.15  # %15'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - daha az overfitting istiyoruz

# Ã–ÄŸrenme seti performans sonuÃ§larÄ±nÄ± tutacak liste
ogrenme_performanslari = []

# Test seti performans sonuÃ§larÄ±nÄ± tutacak liste
test_performanslari = []

# Zaman Ã¶lÃ§Ã¼mÃ¼ baÅŸlat
start_time = time.time()

print("ğŸš€ MODEL EÄÄ°TÄ°MÄ° BAÅLADI")
print("------------------------")
print(f"ğŸ“‚ Excel klasÃ¶rÃ¼: {excel_folder}")
print(f"ğŸ¯ Hedef doÄŸruluk aralÄ±ÄŸÄ±: %{TARGET_ACCURACY_MIN*100:.0f}-%{TARGET_ACCURACY_MAX*100:.0f}")
print(f"ğŸ” Maksimum kabul edilebilir eÄŸitim-test farkÄ±: %{MAX_ACC_DIFF*100:.0f}")

# Excel dosyalarÄ±nÄ± al
try:
    excel_files = [f for f in os.listdir(excel_folder) if f.endswith('.xlsx') or f.endswith('.xls')]
    if not excel_files:
        print("âš ï¸ KlasÃ¶rde Excel dosyasÄ± bulunamadÄ±!")
        exit()
    print(f"\nğŸ“ Toplam iÅŸlenecek dosya sayÄ±sÄ±: {len(excel_files)}\n")
except Exception as e:
    print(f"âš ï¸ KlasÃ¶r okuma hatasÄ±: {str(e)}")
    exit()

# Metin temizleme fonksiyonu
def clean_text(text):
    if not isinstance(text, str):
        return ""
    # KÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼r
    text = text.lower()
    # RakamlarÄ± kaldÄ±r
    text = re.sub(r'\d+', '', text)
    # Ã–zel karakterleri kaldÄ±r
    text = re.sub(r'[^\w\s]', '', text)
    # Fazla boÅŸluklarÄ± temizle
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Dense Transformer - lambda kullanmadan
dense_transformer = FunctionTransformer(make_dense, accept_sparse=True)

# 7 algoritmalÄ± ensemble model oluÅŸtur (daha gÃ¼Ã§lÃ¼ regularizasyon parametreleri)
models = [
    # En iyi genelleme yapan ve en az overfitting gÃ¶steren 7 model - GaussianNB kaldÄ±rÄ±ldÄ±
    ('rf', RandomForestClassifier(min_samples_leaf=5, min_samples_split=10, max_depth=8, n_estimators=100, random_state=42)),
    ('et', ExtraTreesClassifier(min_samples_leaf=5, min_samples_split=10, max_depth=8, n_estimators=100, random_state=42)),
    ('bagging', BaggingClassifier(n_estimators=100, max_samples=0.7, max_features=0.7, random_state=42)),
    ('lr', LogisticRegression(C=0.2, max_iter=1000, penalty='l2', random_state=42)),  # C deÄŸeri daha da dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
    ('svm', SVC(C=0.2, probability=True, kernel='linear', random_state=42)),  # C deÄŸeri daha da dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
    ('mnb', make_pipeline(dense_transformer, MultinomialNB(alpha=2.0))),  # Alpha deÄŸeri daha da artÄ±rÄ±ldÄ±
    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.03, subsample=0.7, 
                                     validation_fraction=0.2, n_iter_no_change=5, random_state=42))  # Early stopping eklendi
]

# Ensemble model
ensemble = VotingClassifier(estimators=models, voting='soft')

# GridSearch iÃ§in daha fazla zaman tanÄ±
print("âš™ï¸ Grid Search Ã§alÄ±ÅŸÄ±yor (daha uzun sÃ¼rebilir)...")

# 10-katlÄ± Ã§apraz doÄŸrulama (daha iyi genelleme iÃ§in)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# GridSearch parametre gridi (dÃ¼ÅŸÃ¼k overfitting iÃ§in daha gÃ¼Ã§lÃ¼ regularizasyon)
param_grid = {
    'rf__n_estimators': [100, 150, 200],
    'rf__max_depth': [5, 6, 8],  # Daha sÄ±nÄ±rlÄ± derinlik (overfitting'i azaltÄ±r)
    'rf__min_samples_leaf': [5, 8, 10],  # Daha yÃ¼ksek deÄŸer (daha gÃ¼Ã§lÃ¼ genelleme)
    'rf__min_samples_split': [10, 15, 20],  # Daha yÃ¼ksek deÄŸer (daha gÃ¼Ã§lÃ¼ genelleme)
    'svm__C': [0.1, 0.2, 0.3],  # Daha dÃ¼ÅŸÃ¼k C deÄŸeri (daha gÃ¼Ã§lÃ¼ regularizasyon)
    'gb__subsample': [0.6, 0.7, 0.8],  # Subsample deÄŸerleri
    'svm__kernel': ['linear']
}

# TF-IDF ile metin Ã¶zellikleÅŸtirme (daha az Ã¶zellik - 400 ile sÄ±nÄ±rlandÄ±rÄ±ldÄ±)
tfidf = TfidfVectorizer(max_features=300, stop_words=turkish_stop_words, ngram_range=(1, 2))

# Her Excel dosyasÄ± iÃ§in model eÄŸitimi yap
for file_idx, file in enumerate(excel_files):
    print(f"\nğŸ”„ Ä°ÅŸleniyor: {file} ({file_idx+1}/{len(excel_files)})")
    
    try:
        # Excel dosyasÄ±nÄ± oku
        file_path = os.path.join(excel_folder, file)
        print(f"   ğŸ“„ Dosya yolu: {file_path}")
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ Dosya bulunamadÄ±: {file_path}")
            continue
            
        try:
            df = pd.read_excel(file_path, engine='openpyxl')
        except Exception as e:
            print(f"âš ï¸ Excel okuma hatasÄ± (openpyxl): {str(e)}")
            print("   ğŸ”„ 'xlrd' motoru ile yeniden deneniyor...")
            try:
                df = pd.read_excel(file_path, engine='xlrd')
            except Exception as e2:
                print(f"âš ï¸ Excel okuma hatasÄ± (xlrd): {str(e2)}")
                print("   â­ï¸ Bu dosya atlanÄ±yor.")
                continue
        
        # Temel kontroller
        if df.empty:
            print(f"âš ï¸ Excel dosyasÄ± boÅŸ: {file}")
            continue
            
        print(f"   â„¹ï¸ SÃ¼tunlar: {', '.join(df.columns.tolist())}")
        
        # Ã–zel kontrol - beklenen sÃ¼tunlarÄ±n varlÄ±ÄŸÄ± ('Tarih', 'Yorum', 'SonuÃ§')
        expected_columns = ['Yorum', 'SonuÃ§']
        
        # SÃ¼tun adlarÄ± ile Ã§alÄ±ÅŸÄ±rken bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf ve boÅŸluk sorunlarÄ± iÃ§in dÃ¼zeltme
        df.columns = df.columns.str.strip()
        column_mapping = {}
        for col in df.columns:
            clean_col = col.lower().strip()
            if clean_col == 'yorum':
                column_mapping[col] = 'Yorum'
            elif clean_col == 'sonuÃ§' or clean_col == 'sonuc':
                column_mapping[col] = 'SonuÃ§'
            elif clean_col == 'tarih':
                column_mapping[col] = 'Tarih'
        
        if column_mapping:
            df = df.rename(columns=column_mapping)
        
        missing_columns = [col for col in expected_columns if col not in df.columns]
        if missing_columns:
            print(f"âš ï¸ UyarÄ±: {file} dosyasÄ±nda gerekli sÃ¼tunlar {', '.join(missing_columns)} bulunamadÄ±!")
            print(f"   Mevcut sÃ¼tunlar: {', '.join(df.columns.tolist())}")
            continue
            
        # NaN deÄŸerleri temizle
        row_count_before = len(df)
        df.dropna(subset=['Yorum', 'SonuÃ§'], inplace=True)
        row_count_after = len(df)
        
        if row_count_before != row_count_after:
            print(f"   â„¹ï¸ {row_count_before - row_count_after} satÄ±r NaN deÄŸerler nedeniyle kaldÄ±rÄ±ldÄ±")
        
        if df.empty:
            print(f"âš ï¸ NaN temizlemeden sonra veri kalmadÄ±: {file}")
            continue
        
        # Veri tiplerini kontrol et
        try:
            # 'SonuÃ§' sÃ¼tununun iÃ§eriÄŸini gÃ¶rÃ¼ntÃ¼le
            print(f"   â„¹ï¸ 'SonuÃ§' sÃ¼tunu deÄŸerleri: {df['SonuÃ§'].value_counts().to_dict()}")
            
            # SonuÃ§ deÄŸerleri 0 ve 1 olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼r
            df['SonuÃ§'] = df['SonuÃ§'].astype(str).str.strip()
            
            # SonuÃ§ deÄŸerlerini dÃ¶nÃ¼ÅŸtÃ¼r - function kullanÄ±mÄ± (lambda yerine)
            def convert_to_binary(x):
                if x.lower() in ['1', 'true', 'evet', 'yes', 'positive', 'pozitif']:
                    return 1
                return 0
                
            df['SonuÃ§'] = df['SonuÃ§'].apply(convert_to_binary)
            print(f"   â„¹ï¸ DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ 'SonuÃ§' sÃ¼tunu deÄŸerleri: {df['SonuÃ§'].value_counts().to_dict()}")
        except Exception as e:
            print(f"âš ï¸ 'SonuÃ§' sÃ¼tunu dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: {str(e)}")
            # FarklÄ± hata ayÄ±klama bilgileri ekleyelim
            print(f"   â„¹ï¸ 'SonuÃ§' sÃ¼tunu veri tipi: {df['SonuÃ§'].dtype}")
            print(f"   â„¹ï¸ Ä°lk 5 'SonuÃ§' deÄŸeri: {df['SonuÃ§'].head().tolist()}")
            continue
        
        # Metin temizleme uygula
        df['Yorum'] = df['Yorum'].apply(clean_text)
        
        # BoÅŸ yorumlarÄ± kaldÄ±r
        df = df[df['Yorum'].str.strip() != '']
        
        # Kategori ismi oluÅŸtur (dosya adÄ±ndan)
        category = file.split('.')[0]
        
        # Veriyi ayÄ±r
        X = df['Yorum'].astype(str)  # String'e dÃ¶nÃ¼ÅŸtÃ¼r
        y = df['SonuÃ§']
        
        print(f"ğŸ“Š Veri seti: Toplam={len(df)}, Pozitif={sum(y)}, Negatif={len(df)-sum(y)}")
        
        # Orta seviye test-train split (%30 test)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)
        
        print(f"   â„¹ï¸ EÄŸitim/test oranÄ±: {len(X_train)}/{len(X_test)} (%{len(X_train)*100/len(X):.0f}/%{len(X_test)*100/len(X):.0f})")
        
        # TF-IDF ile metin Ã¶zellikleÅŸtirme (daha az Ã¶zellik - 400 ile sÄ±nÄ±rlandÄ±rÄ±ldÄ±)
        tfidf = TfidfVectorizer(max_features=300, stop_words=turkish_stop_words, ngram_range=(1, 2))
        X_train_vec = tfidf.fit_transform(X_train)
        X_test_vec = tfidf.transform(X_test)
        
        print(f"   â„¹ï¸ Ã–zellik sayÄ±sÄ±: {X_train_vec.shape[1]}")
        
        # SMOTE ile dengeleme
        try:
            smote = SMOTE(random_state=42)
            X_train_res, y_train_res = smote.fit_resample(X_train_vec, y_train)
            print(f"   â„¹ï¸ SMOTE sonrasÄ± eÄŸitim seti: Pozitif={sum(y_train_res)}, Negatif={len(y_train_res)-sum(y_train_res)}")
        except Exception as e:
            print(f"   âš ï¸ SMOTE hatasÄ±: {str(e)}")
            X_train_res, y_train_res = X_train_vec, y_train
            print("   â„¹ï¸ SMOTE kullanÄ±lamadÄ±, orijinal veri kullanÄ±lÄ±yor.")
        
        # GridSearch iÃ§in daha fazla zaman tanÄ±
        print("âš™ï¸ Grid Search Ã§alÄ±ÅŸÄ±yor (daha uzun sÃ¼rebilir)...")
        
        try:
            # GridSearch'e maksimum 5 dakika tanÄ±
            grid_search_start = time.time()
            grid_search_max_time = 5 * 60  # 5 dakika
            
            print(f"   â„¹ï¸ GridSearch iÃ§in maksimum sÃ¼re: 5 dakika")
            
            # GridSearch iÃ§in timeout kontrolÃ¼
            def timeout_handler():
                if time.time() - grid_search_start > grid_search_max_time:
                    print("   âš ï¸ GridSearch zaman aÅŸÄ±mÄ± - iÅŸlem durduruldu")
                    return True
                return False
            
            # GridSearch daha kÄ±sa sÃ¼rede tamamlanmasÄ± iÃ§in
            param_sample = {}
            for param, values in param_grid.items():
                # DeÄŸerlerin sayÄ±sÄ±nÄ± azalt (en fazla 2 deÄŸer)
                param_sample[param] = values[:2] if len(values) > 2 else values
            
            # KÃ¼Ã§Ã¼ltÃ¼lmÃ¼ÅŸ parametre seti ile GridSearch
            grid_search = GridSearchCV(ensemble, param_sample, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)
            
            try:
                # GridSearch Ã§alÄ±ÅŸtÄ±r, zaman aÅŸÄ±mÄ± kontrolÃ¼ yap
                grid_search.fit(X_train_res, y_train_res)
            except Exception as timeout_e:
                print(f"   âš ï¸ GridSearch iÅŸlemi yarÄ±da kesildi: {str(timeout_e)}")
                # Default modele geri dÃ¶n
                best_model = ensemble
                best_model.fit(X_train_res, y_train_res)
                best_model_name = "ensemble"
                print("   â„¹ï¸ Default ensemble model kullanÄ±lÄ±yor.")
            else:
                grid_search_time = time.time() - grid_search_start
                print(f"   â„¹ï¸ GridSearch sÃ¼resi: {grid_search_time/60:.2f} dakika")
                
                best_model = grid_search.best_estimator_
                best_model_name = "ensemble"
        except Exception as e:
            print(f"âš ï¸ GridSearch hatasÄ±: {str(e)}")
            print("   ğŸ”„ Daha basit model kullanÄ±lacak...")
            
            # Basit model ile devam et
            best_model = models[0][1]  # RandomForest
            best_model.fit(X_train_res, y_train_res)
            best_model_name = "ensemble"  # Her durumda ensemble olarak adlandÄ±r
        
        # EÄŸitim setinde performans kontrolÃ¼
        y_train_pred = best_model.predict(X_train_res)
        train_acc = accuracy_score(y_train_res, y_train_pred)
        
        # Test setinde performans kontrolÃ¼ (overfitting deÄŸerlendirmesi iÃ§in)
        y_test_pred = best_model.predict(X_test_vec)
        test_acc = accuracy_score(y_test, y_test_pred)
        
        print(f"   â„¹ï¸ Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}")
        print(f"   â„¹ï¸ Accuracy FarkÄ±: {train_acc - test_acc:.4f}")
        
        # Overfitting deÄŸerlendirmesi
        if train_acc - test_acc > MAX_ACC_DIFF:
            print(f"   âš ï¸ DÄ°KKAT: YÃ¼ksek overfitting! EÄŸitim-Test farkÄ± %{(train_acc-test_acc)*100:.1f}")
        else:
            print(f"   âœ… Makul genelleme: EÄŸitim-Test farkÄ± %{(train_acc-test_acc)*100:.1f}")
        
        # Performans metrikleri
        train_prec = precision_score(y_train_res, y_train_pred, average='weighted', zero_division=0)
        train_rec = recall_score(y_train_res, y_train_pred, average='weighted', zero_division=0)
        train_f1 = f1_score(y_train_res, y_train_pred, average='weighted', zero_division=0)
        
        test_prec = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)
        test_rec = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)
        test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)
        
        # Ã–ÄŸrenme seti performanslarÄ±nÄ± kaydet
        ogrenme_performanslari.append({
            'Kategori': category,
            'Model': best_model_name,
            'Accuracy': train_acc,
            'Precision': train_prec,
            'Recall': train_rec,
            'F1': train_f1
        })
        
        # Test seti performanslarÄ±nÄ± kaydet
        test_performanslari.append({
            'Kategori': category,
            'Model': best_model_name,
            'Accuracy': test_acc,
            'Precision': test_prec,
            'Recall': test_rec,
            'F1': test_f1,
            'Acc_Fark': train_acc - test_acc
        })
        
        # Modeli kaydet
        model_filename = f"model_{category}.pkl"
        joblib.dump({'model': best_model, 'vectorizer': tfidf}, model_filename)
        print(f"ğŸ’¾ Model kaydedildi: {model_filename}")
        
        print(f"âœ… {category} iÃ§in model eÄŸitildi")
        print(f"   â¤ Ã–ÄŸrenme Seti PerformansÄ±:")
        print(f"     â—† Accuracy:  {train_acc:.4f}")
        print(f"     â—† Precision: {train_prec:.4f}")
        print(f"     â—† Recall:    {train_rec:.4f}")
        print(f"     â—† F1-Score:  {train_f1:.4f}")
        print(f"   â¤ Test Seti PerformansÄ±:")
        print(f"     â—† Accuracy:  {test_acc:.4f}")
        print(f"     â—† Precision: {test_prec:.4f}")
        print(f"     â—† Recall:    {test_rec:.4f}")
        print(f"     â—† F1-Score:  {test_f1:.4f}")
        print(f"   â¤ Overfitting DeÄŸerlendirmesi:")
        print(f"     â—† Accuracy FarkÄ±: {train_acc - test_acc:.4f}")
        
    except Exception as e:
        print(f"âš ï¸ {file} dosyasÄ± iÅŸlenirken hata oluÅŸtu: {str(e)}")
        import traceback
        print(traceback.format_exc())  # DetaylÄ± hata mesajÄ±nÄ± yazdÄ±r

# Ã–ÄŸrenme ve test seti performanslarÄ±nÄ± Excel'e kaydet
try:
    if ogrenme_performanslari:
        performans_df = pd.DataFrame(ogrenme_performanslari)
        performans_df.to_excel("ogrenme_seti_performanslari.xlsx", index=False)
        print("\nâœ… Ã–ÄŸrenme seti performans metrikleri 'ogrenme_seti_performanslari.xlsx' dosyasÄ±na kaydedildi.")
    
    if test_performanslari:
        test_performans_df = pd.DataFrame(test_performanslari)
        test_performans_df.to_excel("test_seti_performanslari.xlsx", index=False)
        print("\nâœ… Test seti performans metrikleri 'test_seti_performanslari.xlsx' dosyasÄ±na kaydedildi.")
        
        # Genel baÅŸarÄ± analizi
        avg_test_acc = test_performans_df['Accuracy'].mean()
        avg_train_acc = performans_df['Accuracy'].mean()
        avg_diff = avg_train_acc - avg_test_acc
        
        print("\nğŸ“Š GENEL PERFORMANS ANALÄ°ZÄ°:")
        print(f"   â†’ Ortalama EÄŸitim DoÄŸruluÄŸu: {avg_train_acc:.4f}")
        print(f"   â†’ Ortalama Test DoÄŸruluÄŸu: {avg_test_acc:.4f}")
        print(f"   â†’ Ortalama Fark: {avg_diff:.4f}")
        
        if avg_diff > MAX_ACC_DIFF:
            print(f"   âš ï¸ DÄ°KKAT: Genel olarak yÃ¼ksek overfitting eÄŸilimi!")
        else:
            print(f"   âœ… Genel olarak makul genelleme")
    
    # Ã–zet tablosu oluÅŸtur ve kaydet
    summary_data = []
    for i in range(len(test_performanslari)):
        summary_data.append({
            'Kategori': test_performanslari[i]['Kategori'],
            'Model': test_performanslari[i]['Model'],
            'Train_Accuracy': ogrenme_performanslari[i]['Accuracy'],
            'Test_Accuracy': test_performanslari[i]['Accuracy'],
            'Accuracy_FarkÄ±': ogrenme_performanslari[i]['Accuracy'] - test_performanslari[i]['Accuracy'],
            'Train_F1': ogrenme_performanslari[i]['F1'],
            'Test_F1': test_performanslari[i]['F1']
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_excel("model_ozet.xlsx", index=False)
    print("\nâœ… Model Ã¶zet tablosu 'model_ozet.xlsx' dosyasÄ±na kaydedildi.")
        
except Exception as e:
    print(f"\nâš ï¸ Excel dosyasÄ± oluÅŸturulurken hata: {str(e)}")
    # Ekrana yazdÄ±r
    if ogrenme_performanslari:
        print("\nğŸ“Š Ã–ÄRENME SETÄ° PERFORMANS METRÄ°KLERÄ°:")
        for perf in ogrenme_performanslari:
            print(f"   â†’ {perf['Kategori']} ({perf['Model']}): Accuracy={perf['Accuracy']:.4f}, F1={perf['F1']:.4f}")
    
    if test_performanslari:
        print("\nğŸ“Š TEST SETÄ° PERFORMANS METRÄ°KLERÄ°:")
        for perf in test_performanslari:
            print(f"   â†’ {perf['Kategori']} ({perf['Model']}): Accuracy={perf['Accuracy']:.4f}, F1={perf['F1']:.4f}, Fark={perf['Acc_Fark']:.4f}")
    else:
        print("\nâš ï¸ HiÃ§bir model eÄŸitilemedi.")

total_time = time.time() - start_time
print(f"\nâ±ï¸ Toplam Ã§alÄ±ÅŸma sÃ¼resi: {total_time/60:.2f} dakika")
print("\nâœ… Ä°ÅŸlem tamamlandÄ±!")
if ogrenme_performanslari:
    print("\nğŸ‘‰ Tahmin yapmak iÃ§in 'tahmin.py' dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rabilirsiniz.") 